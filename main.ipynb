{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0aa1577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (21.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (4.61.2)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy) (49.6.0.post20210108)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.1)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Using cached thinc-8.1.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: typing-extensions, catalogue, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-gpu 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 smart-open-6.3.0 spacy-3.5.2 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 typer-0.7.0 typing-extensions-4.5.0 wasabi-1.1.1\n",
      "2023-04-14 17:56:12.812957: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/pkgs/cudatoolkit-11.2.2-he111cf0_8/lib/:\n",
      "2023-04-14 17:56:12.813021: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-14 17:56:12.813271: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dsmlp-jupyter-smodafferi): /proc/driver/nvidia/version does not exist\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 11.1 MB/s eta 0:00:01    |█████████                       | 3.6 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/conda/lib/python3.9/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.61.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.6)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (0.24.2)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.7.0)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.0.1\n",
      "    Uninstalling joblib-1.0.1:\n",
      "      Successfully uninstalled joblib-1.0.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e38ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 17:56:35.691445: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/pkgs/cudatoolkit-11.2.2-he111cf0_8/lib/:\n",
      "2023-04-14 17:56:35.691514: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-14 17:56:35.691553: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dsmlp-jupyter-smodafferi): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict \n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import re\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56eddccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('ted_talks_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b6730ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = raw['topics'].to_numpy()\n",
    "keywords = defaultdict(int)\n",
    "for i, inner in enumerate(arr):\n",
    "    x = inner.split(\"'\")\n",
    "    for i, tag in enumerate(x):\n",
    "        if i % 2 != 0 and i != 0 and i != len(x) - 1:\n",
    "            keywords[tag.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b55891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_keywords = sorted(list(keywords.keys()), key=lambda keyword: keywords[keyword])[::-1]\n",
    "occurences = [keywords[keyword] for keyword in most_common_keywords]\n",
    "#list(zip(most_common_keywords, occurences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdbd0e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([176., 103.,  45.,  31.,  25.,  10.,  10.,  10.,   7.,   3.,   5.,\n",
       "          6.,   2.,   2.,   1.,   1.,   3.,   2.,   1.,   2.,   1.,   0.,\n",
       "          2.,   0.,   1.,   2.,   0.,   1.,   2.,   1.,   0.,   0.,   0.,\n",
       "          0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   1.]),\n",
       " array([  0,  20,  40,  60,  80, 100, 120, 140, 160, 180, 200, 220, 240,\n",
       "        260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500,\n",
       "        520, 540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760,\n",
       "        780, 800, 820, 840, 860, 880, 900, 920, 940, 960, 980]),\n",
       " <BarContainer object of 49 artists>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(occurences, bins=np.arange(0, 1000, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02d2955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of topics with occurences >= 50 : 156\n"
     ]
    }
   ],
   "source": [
    "cutoff = 50\n",
    "y = filter(lambda x: x >= cutoff, occurences)\n",
    "print(f\"Amount of topics with occurences >= {cutoff} : {len(list(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60168131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_topics(topic):\n",
    "    x = topic.split(\"'\")\n",
    "    tags = []\n",
    "    for i, tag in enumerate(x):\n",
    "        if i % 2 != 0 and i != 0 and i != len(x) - 1:\n",
    "            tags.append(tag.lower())\n",
    "    return tags\n",
    "\n",
    "def correct_dataframe_topics_column(df, in_place=False):\n",
    "    if not in_place:\n",
    "        df = df.copy()\n",
    "    topics_listed_column = df['topics'].apply(convert_topics)\n",
    "    df['topics'] = topics_listed_column\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04e8173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Topics of focus:\n",
    "STEM - Science, Technology, Engineering, Math(Mathematics), Psychology, Physics, Biotech, AI, Cognitive Science\n",
    "Culture - Culture, Society, Community\n",
    "Business - Business, Economics\n",
    "Social Change - social change/activism\n",
    "History - History/Politics\n",
    "\"\"\"\n",
    "topic_to_group = defaultdict(lambda: 'not-relevant')\n",
    "stem = [\"science\", \"technology\", \"engineering\", \"math\", \"mathematics\", \"psychology\", \"physics\", \"biotech\", \"ai\", \"cognitive science\", 'math', 'computers', 'science', 'software', 'technology', 'visualizations', 'health']\n",
    "culture = [\"culture\", \"society\", \"community\", 'culture', 'humor', 'visualizations', 'children', 'creativity', 'entertainment', 'media', 'education', 'parenting', 'teaching', 'collaboration', 'music', 'performance', 'activism', 'africa', 'inequality', 'politics']\n",
    "business = [\"business\", \"economics\",  'design', 'comedy', 'storytelling', 'entertainment', 'media', 'business', 'cities', 'demo', 'economics']\n",
    "activism = ['social change', 'activism',  'climate change', 'environment', 'global issues', 'sustainability', 'green', 'inequality', 'global development', 'pollution']\n",
    "history = ['history', 'politics',  'creativity', 'architecture', 'religion', 'storytelling']\n",
    "categories = [ ['stem'] + stem, ['culture'] + culture, ['business'] + business,\n",
    "              ['activism'] + activism, ['history'] + history]\n",
    "\n",
    "for category in categories:\n",
    "    cat = None\n",
    "    for i, topic in enumerate(category):\n",
    "        if i == 0:\n",
    "            cat = topic\n",
    "            continue\n",
    "        topic_to_group[topic] = cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb26c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_topics(topic_list):\n",
    "    grouped = set()\n",
    "    for topic in topic_list:\n",
    "        grouped.add(topic_to_group[topic])\n",
    "    if len(grouped) > 1 and 'not-relevant' in grouped:\n",
    "        grouped.remove('not-relevant')\n",
    "    return grouped\n",
    "\n",
    "def group_df_topics(df, in_place=False):\n",
    "    if not in_place:\n",
    "        df = df.copy()\n",
    "    df['grouped_topics'] = df['topics'].apply(group_topics)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee1b0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_words(script): \n",
    "    unique_words = dict()\n",
    "    script_lst = script.split()\n",
    "    for word in script_lst: \n",
    "        if word in unique_words: \n",
    "            unique_words[word] += 1\n",
    "        else: \n",
    "            unique_words[word] = 1\n",
    "    return unique_words\n",
    "\n",
    "def add_words(df, in_place=False):\n",
    "    if in_place:\n",
    "        df = df.copy()\n",
    "    df[\"clean_scripts\"] = (\n",
    "    df[\"transcript\"]\n",
    "    .str.replace(r'\\(Music: [^)]+\\) ','', regex = True)\n",
    "    .str.replace(r' — ', ' ', regex = True)\n",
    "    .str.replace(r'[[.\"\\'!\\],:?;=+*&]', '', regex = True)\n",
    "    .str.lower())\n",
    "    df[\"words\"] = df[\"clean_scripts\"].apply(count_unique_words)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b67ae7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df: pd.DataFrame, verbose=False, in_place=False):\n",
    "    \"\"\"\n",
    "    expects df to have a columns ['words'] which contains a dictionary for each entry,\n",
    "    this dictionary has all the unique words in this row's transcript as keys, and their values\n",
    "    correspond the amount of time that word appears in the transcript. This creates \n",
    "    \"\"\"\n",
    "    word_dicts = df['words'].to_numpy()\n",
    "    total_dict = defaultdict(int)\n",
    "    total_count = 0\n",
    "    for word_dict in word_dicts:\n",
    "        for word in word_dict.keys():\n",
    "            total_dict[word] += word_dict[word]\n",
    "            total_count += word_dict[word]\n",
    "    unique_count = len(list(total_dict.keys()))\n",
    "    if verbose: \n",
    "        print(\"Amount of unique words: {:,}\".format(unique_count), \n",
    "             \"\\nTotal word count: {:,}\".format(total_count))\n",
    "    \n",
    "    word_to_onehot_location = {}\n",
    "    for i, word in enumerate(list(total_dict.keys())):\n",
    "        word_to_onehot_location[word] = i\n",
    "        \n",
    "    new_col = []\n",
    "    for i, row in enumerate(word_dicts):\n",
    "        new_entry = np.zeros(unique_count, dtype=int)\n",
    "        for word in row:\n",
    "            new_entry[word_to_onehot_location[word]] = 1\n",
    "        new_col.append(new_entry)\n",
    "    if in_place:\n",
    "        new_df = df\n",
    "    else:\n",
    "        new_df = df.copy()\n",
    "    new_df['one-hot'] = new_col\n",
    "    return new_df\n",
    "\n",
    "def sentence_type(df, in_place=False):\n",
    "    \"\"\"\n",
    "    adds an array to each row\n",
    "    this array contains the frequency of each setence type\n",
    "    arr[0] = . | arr[1] = ? | arr[2] = !\n",
    "    each entry is total occurences / total punctuation count\n",
    "    \"\"\"\n",
    "    new_col = []\n",
    "    for transcript in df['transcript']:\n",
    "        new_entry = np.zeros(3)\n",
    "        p_count, q_count, e_count = transcript.count('.'), transcript.count('?'), transcript.count('!')\n",
    "        total_sentences = p_count + q_count + e_count + 1\n",
    "        new_entry[0] = (p_count + 1)/ total_sentences\n",
    "        new_entry[1] = (q_count + 1)/ total_sentences\n",
    "        new_entry[2] = (e_count + 1)/ total_sentences\n",
    "        new_col.append(new_entry)\n",
    "    if in_place:\n",
    "        new_df = df\n",
    "    else:\n",
    "        new_df = df.copy()\n",
    "    new_df['sentence-type'] = new_col\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3263e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TFIDF Calculation\n",
    "def tfidf_value(transcript):\n",
    "    tfIdfVectorizer = TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform([transcript])\n",
    "    #list_of_words_tfidf = tfIdfVectorizer.get_feature_names_out()\n",
    "    tfidf_value = tfIdf.toarray()[0]\n",
    "    return tfidf_value\n",
    "\n",
    "def tfidf_words(transcript):\n",
    "    tfIdfVectorizer = TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform([transcript])\n",
    "    try:\n",
    "        list_of_words_tfidf = tfIdfVectorizer.get_feature_names_out()\n",
    "    except Error:\n",
    "        list_of_words_tfidf = tfIdfVectorizer.get_feature_names()\n",
    "    tfidf_words = list_of_words_tfidf\n",
    "    return tfidf_words\n",
    "\n",
    "def tfidf_dict(transcript):\n",
    "    tfIdfVectorizer = TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform([transcript])\n",
    "    try:\n",
    "        list_of_words_tfidf = tfIdfVectorizer.get_feature_names_out()\n",
    "    except Error:\n",
    "        list_of_words_tfidf = tfIdfVectorizer.get_feature_names()\n",
    "    tfidf_value = tfIdf.toarray()[0]\n",
    "    tfidf_dict = dict(zip(list_of_words_tfidf,tfidf_value))\n",
    "    return tfidf_dict\n",
    "\n",
    "def add_tfidf(df: pd.DataFrame, in_place=False):\n",
    "    if in_place:\n",
    "        df = df.copy()\n",
    "    df['tfidf'] = df['transcript'].apply(tfidf_value)\n",
    "    df['tfidf_word'] = df['transcript'].apply(tfidf_words)\n",
    "    df['tfidf_dict'] = df['transcript'].apply(tfidf_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e914aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_engineered():\n",
    "    data = raw.copy()\n",
    "    data = correct_dataframe_topics_column(data, in_place=True)\n",
    "    data = group_df_topics(data, in_place=True)\n",
    "    data = add_words(data, in_place=True)\n",
    "    data = one_hot_encode(data, in_place=True)\n",
    "    data = sentence_type(data, in_place=True)\n",
    "    data = add_tfidf(data, in_place=True)\n",
    "    data = data.drop(columns = [\n",
    "    \"talk_id\", \"speaker_1\", \"views\", \"recorded_date\", \"published_date\", \n",
    "    \"event\", \"duration\", \"url\", \"comments\", \"about_speakers\", \n",
    "    \"available_lang\", \"all_speakers\", \"native_lang\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abe3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_feature_engineered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = pd.Series(keywords).to_frame().reset_index().rename(columns = {\"index\": \"category\", 0:\"count\"})\n",
    "\n",
    "df_count = df_count[df_count[\"count\"] >= 50]\n",
    "total_count = df_count.shape[0]\n",
    "\n",
    "lst = []\n",
    "for i in range(0, total_count, 39):\n",
    "    df_cut = df_count.iloc[i: i+ 39]\n",
    "    lst.append(df_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec024c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7193f84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762b0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
